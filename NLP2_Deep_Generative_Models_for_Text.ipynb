{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wR5bKeAqkhhy"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "X19E1QK6lJ0H",
    "outputId": "d7a01a73-4dfc-46c9-c99b-55126839317f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f5b4fcb86a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(0,10,1000)\n",
    "y = np.sin(x)\n",
    "ax.scatter(x,y, c = x, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 911
    },
    "colab_type": "code",
    "id": "V1kU1hQW39JR",
    "outputId": "6ccbc821-0ce1-4500-a03c-58df57f5cc0f"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive/')\n",
    "#%cd \"/content/drive/My Drive/NLP2/NLP2_Deep_Generative_Models_for_Text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jZeZcmqvkDWm"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AFFRDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of items in the dataset\"\"\"\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the datapoint at index i as a tuple (sentence, label),\n",
    "        where the sentence is tokenized.\n",
    "        \"\"\"\n",
    "        encoded = self.tokenizer.encode(\n",
    "            self.sentences[idx], add_special_tokens=True)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "I4lIX2xH7zT2",
    "outputId": "a1796de5-adce-4ba0-e9b8-88cc8630d0e5"
   },
   "outputs": [],
   "source": [
    "from tokenizers import WordTokenizer\n",
    "\n",
    "# Create the datasets\n",
    "\n",
    "# Train your tokenizer.\n",
    "reader = nltk.corpus.BracketParseCorpusReader(\".\", \"02-21.10way.clean\")\n",
    "tree = reader.parsed_sents()[:]\n",
    "text = [\" \".join(line.leaves()).lower() for line in tree]\n",
    "tokenizer = WordTokenizer(text, max_vocab_size=10000)\n",
    "EOS = tokenizer.encode('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DXt87J47l5ur"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: in an oct. 19 review of `` the misanthrope '' at chicago 's goodman theatre -lrb- `` revitalized classics take the stage in windy city , '' leisure & arts -rrb- , the role of celimene , played by kim cattrall , was mistakenly attributed to christina haag .\n",
      "tokenized: [4754, 926, 6325, 185, 7745, 6332, 584, 9060, 3, 10, 1161, 2002, 16, 4273, 9063, 22, 584, 3, 3, 8917, 9060, 8556, 4754, 3, 2067, 18, 10, 5363, 8, 1113, 24, 18, 9060, 7826, 6332, 3, 18, 6822, 1744, 5193, 3, 18, 9725, 5944, 1200, 9167, 3, 3, 25]\n",
      "decoded: in an oct. 19 review of `` the [UNK] '' at chicago 's goodman theatre -lrb- `` [UNK] [UNK] take the stage in [UNK] city , '' leisure & arts -rrb- , the role of [UNK] , played by kim [UNK] , was mistakenly attributed to [UNK] [UNK] .\n",
      "\n",
      "original: ms. haag plays elianti .\n",
      "tokenized: [6042, 3, 6826, 3, 25]\n",
      "decoded: ms. [UNK] plays [UNK] .\n",
      "\n",
      "original: rolls-royce motor cars inc. said it expects its u.s. sales to remain steady at about 1,200 cars in 1990 .\n",
      "tokenized: [3, 6021, 1860, 4760, 7923, 5028, 3647, 5034, 9388, 7929, 9167, 7554, 8603, 1161, 609, 41, 1860, 4754, 226, 25]\n",
      "decoded: [UNK] motor cars inc. said it expects its u.s. sales to remain steady at about 1,200 cars in 1990 .\n",
      "\n",
      "original: the luxury auto maker last year sold 1,214 cars in the u.s.\n",
      "tokenized: [9060, 5569, 1222, 5620, 5281, 9963, 8400, 3, 1860, 4754, 9060, 9388]\n",
      "decoded: the luxury auto maker last year sold [UNK] cars in the u.s.\n",
      "\n",
      "original: howard mosher , president and chief executive officer , said he anticipates growth for the luxury auto maker in britain and europe , and in far eastern markets .\n",
      "tokenized: [4638, 3, 18, 6996, 936, 2005, 3617, 6341, 18, 7923, 4466, 977, 4348, 3991, 9060, 5569, 1222, 5620, 4754, 1640, 936, 3551, 18, 936, 4754, 3760, 3297, 5687, 25]\n",
      "decoded: howard [UNK] , president and chief executive officer , said he anticipates growth for the luxury auto maker in britain and europe , and in far eastern markets .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if everything works.\n",
    "for sentence in text[:5]:\n",
    "    tokenized = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "    sentence_decoded = tokenizer.decode(tokenized)\n",
    "    print('original:', sentence)\n",
    "    print('tokenized:', tokenized)\n",
    "    print('decoded:', sentence_decoded)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SQTuOkW4WTt"
   },
   "outputs": [],
   "source": [
    "files = [\"02-21.10way.clean\", \"22.auto.clean\", \"23.auto.clean\"]\n",
    "\n",
    "# Get output dict and remapping of file names to dataset names\n",
    "data_dict = {}\n",
    "remap_dict = {\"02-21.10way.clean\":\"train\", \n",
    "              \"22.auto.clean\":\"validation\",\n",
    "              \"23.auto.clean\":\"test\"}\n",
    "\n",
    "# Parse the data with nltk bracket parser\n",
    "for file in files:\n",
    "    reader = nltk.corpus.BracketParseCorpusReader(\".\", file)\n",
    "    tree = reader.parsed_sents()[:]\n",
    "    \n",
    "    # Assign a dataset dict to train, validation or test\n",
    "    data_dict[remap_dict[file]] = [\" \".join(line.leaves()).lower() for line in tree]\n",
    "\n",
    "train_data = AFFRDataset(data_dict['train'], tokenizer)\n",
    "validation_data = AFFRDataset(data_dict['validation'], tokenizer)\n",
    "test_data = AFFRDataset(data_dict['test'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORk2CU_8qYfA"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextGenerationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, gru_num_hidden=256, gru_num_layers=2, device='cuda:0',dropout=0):\n",
    "        super(TextGenerationModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,256)\n",
    "        self.gru = nn.GRU(256,gru_num_hidden,gru_num_layers,dropout=dropout,batch_first=True)\n",
    "        self.output = nn.Linear(gru_num_hidden,vocab_size)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        input = x.to(self.device)\n",
    "        embed = self.embed(input)\n",
    "        if(hidden == None):\n",
    "            out,hidden = self.gru.forward(embed)\n",
    "        else:\n",
    "            out,hidden = self.gru.forward(embed,hidden)\n",
    "        out = self.output(out)\n",
    "\n",
    "        return out, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hC-pZ4WCnXPB"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def padded_collate(batch):\n",
    "    \"\"\"Pad sentences, return sentences and labels as LongTensors.\"\"\"\n",
    "    sentences = [sentence[:-1] for sentence in batch]\n",
    "    labels = [sentence[1:] for sentence in batch]\n",
    "\n",
    "    lengths = [len(s) for s in sentences]\n",
    "    max_length = max(lengths)\n",
    "    # Pad each sentence with zeros to max_length\n",
    "    padded_sents = [s + [0] * (max_length - len(s)) for s in sentences]\n",
    "    padded_labels = [s + [0] * (max_length - len(s)) for s in labels]\n",
    "    return torch.LongTensor(padded_sents), torch.LongTensor(padded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RWgqPpkpo7jL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(config, train_data, valid_data):\n",
    "\n",
    "    # Initialize the device which to run the model on\n",
    "    if(torch.cuda.is_available()):\n",
    "        device = torch.device(\"cuda\")\n",
    "        print('Device = CUDA')\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print('Device = CPU')\n",
    "        \n",
    "    #Paths to save the model and optimizer to\n",
    "    modelpath = config['model_path']\n",
    "    optimpath = config['optim_path']\n",
    "\n",
    "    # Initialize the model that we are going to use\n",
    "    makeNew = config['new_model']\n",
    "\n",
    "    #Load in model if necessary\n",
    "    if(not makeNew and modelpath != \"\"):\n",
    "        model = (torch.load(modelpath))\n",
    "    else:\n",
    "        model = TextGenerationModel(config['vocab_size'],config['num_hidden'],config['num_layers'],device)\n",
    "\n",
    "    # Setup the loss and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(),config['learning_rate'])\n",
    "\n",
    "    #Load in the optimizer if necessary\n",
    "    if(not makeNew and optimpath != \"\"):\n",
    "        optimizer.load_state_dict(torch.load(optimpath))\n",
    "\n",
    "    accs = []\n",
    "    losses = []\n",
    "    curr_accs = []\n",
    "    curr_losses = []\n",
    "    print_steps = []\n",
    "    convergence = False\n",
    "    conv_count = 0\n",
    "    prev_loss = np.inf\n",
    "\n",
    "    iteration = 0\n",
    "    epoch = 0\n",
    "    while(not convergence):\n",
    "        print(\"Epoch: \" + str(epoch))\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        for step, (batch_inputs, batch_targets) in enumerate(train_data):\n",
    "            # Only for time measurement of step through network\n",
    "            t1 = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            targets = batch_targets.to(device)\n",
    "            out,_ = model.forward(batch_inputs)\n",
    "\n",
    "            #Calculate loss and accuracy\n",
    "            curr_loss = 0\n",
    "            curr_acc = 0\n",
    "            cor = 0\n",
    "\n",
    "            seq_length = batch_inputs.shape[1]\n",
    "\n",
    "            # Get average original lenghts of sentences to divide total loss by\n",
    "            orig_lengths = torch.mean(torch.argmin(batch_inputs,dim=1).float())\n",
    "\n",
    "            for i in range(seq_length):\n",
    "                out_t = out[:,i,:]\n",
    "                targets_t = targets[:,i]\n",
    "                curr_loss += criterion.forward(out_t, targets_t)\n",
    "                preds = (torch.argmax(out_t,dim=1)).long()\n",
    "                cor += targets_t.eq(preds).sum().item()\n",
    "            curr_acc = cor/(seq_length * targets.size()[0])\n",
    "            loss += curr_loss.item()\n",
    "            accuracy += curr_acc\n",
    "\n",
    "            curr_loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config['max_norm'])\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            curr_accs.append(curr_acc)\n",
    "            curr_losses.append(curr_loss.item())\n",
    "\n",
    "            # Just for time measurement\n",
    "            t2 = time.time()\n",
    "            examples_per_second = config['batch_size']/float(t2-t1)\n",
    "            if(iteration % config['print_every'] == 0):\n",
    "                loss_std = np.std(curr_losses)\n",
    "                print(\"[{}] Epoch {:04d}, Train Step {:04d}, Batch Size = {}, Examples/Sec = {:.2f}, \"\n",
    "                    \"Avg. Accuracy = {:.2f}, Avg. Loss = {:.3f}, Loss STD = {:.3f}\".format(\n",
    "                        datetime.now().strftime(\"%Y-%m-%d %H:%M\"), epoch, iteration,\n",
    "                        config['batch_size'], examples_per_second,\n",
    "                        accuracy/config['print_every'], loss/config['print_every'], loss_std\n",
    "                ))\n",
    "                accs.append(accuracy/config['print_every'])\n",
    "                losses.append(loss/config['print_every'])\n",
    "                print_steps.append(iteration)\n",
    "                if(np.abs(prev_loss/config['print_every'] - loss/config['print_every']) < 0.001):\n",
    "                    conv_count += 1\n",
    "                else:\n",
    "                    conv_count = 0\n",
    "                convergence = conv_count == 5\n",
    "                prev_loss = loss\n",
    "                accuracy = 0\n",
    "                loss = 0\n",
    "            #Generate text\n",
    "            if iteration % config['sample_every'] == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    text = generate_text(model,device,config['sample_strat'],config['sample_temp'])\n",
    "                    print(str(iteration), \": \", text)\n",
    "                model.train()\n",
    "                '''                if(modelpath != \"\"):\n",
    "                    torch.save(model,modelpath)\n",
    "                if(optimpath != \"\"):\n",
    "                    torch.save(optimizer.state_dict(),optimpath)'''\n",
    "            iteration += 1\n",
    "\n",
    "        #TODO: Validate\n",
    "        model.eval()\n",
    "        iter = 0\n",
    "        for step, (batch_inputs, batch_targets) in enumerate(valid_data):\n",
    "            targets = batch_targets.to(device)\n",
    "            out,_ = model.forward(batch_inputs)\n",
    "            #Calculate loss and accuracy\n",
    "            curr_loss = 0\n",
    "            curr_acc = 0\n",
    "            cor = 0\n",
    "\n",
    "            seq_length = batch_inputs.shape[1]\n",
    "\n",
    "            # Get average original lenghts of sentences to divide total loss by\n",
    "            orig_lengths = torch.mean(torch.argmin(batch_inputs,dim=1).float())\n",
    "\n",
    "            for i in range(seq_length):\n",
    "                out_t = out[:,i,:]\n",
    "                targets_t = targets[:,i]\n",
    "                curr_loss += criterion.forward(out_t, targets_t)\n",
    "                preds = (torch.argmax(out_t,dim=1)).long()\n",
    "                cor += targets_t.eq(preds).sum().item()\n",
    "            curr_acc = cor/(seq_length * targets.size()[0])\n",
    "            loss += curr_loss.item()\n",
    "            accuracy += curr_acc\n",
    "            curr_accs.append(curr_acc)\n",
    "            curr_losses.append(curr_loss.item())\n",
    "            iter += 1\n",
    "        loss_std = np.std(curr_losses)\n",
    "        print(\"Epoch {:04d}, Validation, Avg. Accuracy = {:.2f}, Avg. Loss = {:.3f}, Loss STD = {:.3f}\".format(\n",
    "                epoch, accuracy/iter, loss/iter, loss_std))\n",
    "        model.train()\n",
    "        epoch += 1\n",
    "        if(epoch == config['epochs'] or convergence):\n",
    "            break\n",
    "\n",
    "    print('Done training.')\n",
    "    print(accs)\n",
    "    print(losses)\n",
    "    print(print_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzJ0VAMeYHSb"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_text(model,device,sampling_strat='max',temperature=1, starting_text=[1],ksize=1):\n",
    "    assert sampling_strat in ('max', 'rand')\n",
    "    \n",
    "    # Start with encoded text\n",
    "    start = np.array(starting_text)\n",
    "    text = list(start) #This stores the eventual output\n",
    "    current = torch.from_numpy(start).long().unsqueeze(dim=0) \n",
    "\n",
    "    #The initial step\n",
    "    input = current.to(device)\n",
    "    output,hidden = model.forward(input)\n",
    "    current = output[0,-1,:].squeeze()\n",
    "    if(sampling_strat == 'max'):\n",
    "        guess = torch.argmax(current).unsqueeze(0)\n",
    "    elif(sampling_strat == 'rand'):\n",
    "        guess = torch.multinomial(F.softmax(temperature*current,dim=0),1)\n",
    "    text.append(guess.item())\n",
    "    input = guess.unsqueeze(0)\n",
    "\n",
    "    #Now that we have an h and c, we can start the loop\n",
    "    i = 0\n",
    "    while(i < 100):\n",
    "        output,hidden = model.forward(input,hidden)\n",
    "        current = output.squeeze()\n",
    "        if(sampling_strat == 'max'):\n",
    "            guess = torch.argmax(current).unsqueeze(0)\n",
    "        elif(sampling_strat == 'rand'):\n",
    "            guess = torch.multinomial(F.softmax(temperature*current,dim=0),1)\n",
    "        text.append(guess.item())\n",
    "        input = guess.unsqueeze(0)\n",
    "        i += 1\n",
    "        if(guess.item() == 2):\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "-B1V5dagp3z9",
    "outputId": "0089d545-cebb-4438-fc3b-0fcf9345f5aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'config = {}\\n\\n# Model params\\nconfig[\\'model_path\\'] = \"models\\\\trump_model.txt\"\\nconfig[\\'optim_path\\']=\"models\\\\trump_optim.txt\"\\nconfig[\\'new_model\\']=True\\nconfig[\\'num_hidden\\']=128\\nconfig[\\'num_layers\\']=2\\nconfig[\\'vocab_size\\']=tokenizer.vocab_size\\n\\n# Training params\\nconfig[\\'batch_size\\']=64\\nconfig[\\'learning_rate\\']=2e-3\\n\\n# It is not necessary to implement the following three params, but it may help training.\\nconfig[\\'learning_rate_decay\\']=0.96\\nconfig[\\'learning_rate_step\\']=5000\\nconfig[\\'dropout_keep_prob\\']=1\\nconfig[\\'epochs\\']=10\\nconfig[\\'max_norm\\']=5.0\\n\\n# Misc params\\nconfig[\\'print_every\\']=100\\nconfig[\\'sample_every\\']=100\\nconfig[\\'sample_strat\\']=\\'rand\\'\\nconfig[\\'sample_temp\\']=1.5\\n\\n# Create dataloaders\\ntrain_loader = DataLoader(train_data, batch_size=config[\\'batch_size\\'], shuffle=True, collate_fn=padded_collate)\\nvalid_loader = DataLoader(validation_data, batch_size=256, shuffle=False, collate_fn=padded_collate)\\ntest_loader = DataLoader(test_data, batch_size=256, shuffle=False, collate_fn=padded_collate)\\n\\n# Train the model\\ntrain(config, train_loader, valid_loader)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''config = {}\n",
    "\n",
    "# Model params\n",
    "config['model_path'] = \"models\\\\trump_model.txt\"\n",
    "config['optim_path']=\"models\\\\trump_optim.txt\"\n",
    "config['new_model']=True\n",
    "config['num_hidden']=128\n",
    "config['num_layers']=2\n",
    "config['vocab_size']=tokenizer.vocab_size\n",
    "\n",
    "# Training params\n",
    "config['batch_size']=64\n",
    "config['learning_rate']=2e-3\n",
    "\n",
    "# It is not necessary to implement the following three params, but it may help training.\n",
    "config['learning_rate_decay']=0.96\n",
    "config['learning_rate_step']=5000\n",
    "config['dropout_keep_prob']=1\n",
    "config['epochs']=10\n",
    "config['max_norm']=5.0\n",
    "\n",
    "# Misc params\n",
    "config['print_every']=100\n",
    "config['sample_every']=100\n",
    "config['sample_strat']='rand'\n",
    "config['sample_temp']=1.5\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True, collate_fn=padded_collate)\n",
    "valid_loader = DataLoader(validation_data, batch_size=256, shuffle=False, collate_fn=padded_collate)\n",
    "test_loader = DataLoader(test_data, batch_size=256, shuffle=False, collate_fn=padded_collate)\n",
    "\n",
    "# Train the model\n",
    "train(config, train_loader, valid_loader)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wSoMf9mCcWsN"
   },
   "source": [
    "# Sentence VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "AN4v_77RcChw",
    "outputId": "a6614ea6-7f54-41f6-8409-2e5ec9e0c7f3"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.14 GiB (GPU 0; 3.95 GiB total capacity; 2.42 GiB already allocated; 1.02 GiB free; 2.44 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-dd77f2ee8949>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0melbos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mtrain_elbo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_elbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melbos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtrain_curve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_elbo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Uni/A.I./NLP2/Project_DGM/SentenceVAE.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, data, optimizer, device)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mval_elbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_elbo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_elbo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Uni/A.I./NLP2/Project_DGM/SentenceVAE.py\u001b[0m in \u001b[0;36mepoch_iter\u001b[0;34m(model, data, optimizer, device)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0mbatch_elbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m                 \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mtotal_elbo\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_elbo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Uni/A.I./NLP2/Project_DGM/SentenceVAE.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, targets, device)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mpx_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mp_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpx_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`logits` parameter must be at least one-dimensional.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.14 GiB (GPU 0; 3.95 GiB total capacity; 2.42 GiB already allocated; 1.02 GiB free; 2.44 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from SentenceVAE import run_epoch\n",
    "from SentenceVAE import SentenceVAE\n",
    "from torch.optim import Adam\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "epochs = 10\n",
    "zdim = 13\n",
    "batch_size = 4\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=padded_collate)\n",
    "valid_loader = DataLoader(validation_data, batch_size=256, shuffle=False, collate_fn=padded_collate)\n",
    "test_loader = DataLoader(test_data, batch_size=256, shuffle=False, collate_fn=padded_collate)\n",
    "\n",
    "model = SentenceVAE(vocab_size, z_dim=zdim) \n",
    "model.to(device)\n",
    "#sampled = model.sample(9,device)[0]\n",
    "\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "train_curve, val_curve = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    elbos = run_epoch(model, (train_loader, valid_loader), optimizer, device)\n",
    "    train_elbo, val_elbo = elbos\n",
    "    train_curve.append(train_elbo)\n",
    "    val_curve.append(val_elbo)\n",
    "    print(f\"[Epoch {epoch}] train elbo: {train_elbo} val_elbo: {val_elbo}\")\n",
    "    \n",
    "    #sampled = model.sample(9, device)[0]\n",
    "\n",
    "save_elbo_plot(train_curve, val_curve, 'elbo.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP2_Deep_Generative_Models_for_Text.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
