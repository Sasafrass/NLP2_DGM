{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DGM_colab.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"rc37xacoeG04","colab_type":"code","outputId":"8a382ead-e7a8-43b7-8005-0ebe22be1d76","executionInfo":{"status":"ok","timestamp":1587763244201,"user_tz":-120,"elapsed":1552,"user":{"displayName":"Aunel","photoUrl":"","userId":"13252118019665450025"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","%cd \"/content/drive/My Drive/NLP2_DGM\"\n","\n","# imports\n","import os\n","import numpy as np\n","import pandas as pd\n","import tkinter\n","\n","# Preprocessing\n","from preprocessing import AFFRDataset, get_data, padded_collate\n","\n","# All things torch-y\n","import torch\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","from torch.optim import Adam\n","\n","# To parse dem arguments\n","import argparse\n","\n","# device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"You're running on:\", device)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","/content/drive/My Drive/NLP2_DGM\n","You're running on: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HHLKec3Ufj95","colab_type":"code","colab":{}},"source":["# Own classes\n","from helpers import generate_text\n","from RNNLM import RNNLM\n","\n","def train_rnnlm(config, train_data, valid_data, tokenizer):\n","    \"\"\"\n","    Args:\n","        config    : Argparse object (turned dictionary) containing all parameters\n","        train_data: Fold with training data\n","        valid_data: Fold with validation data\n","        tokenizer : Object to tokenize words with\n","    \"\"\"\n","\n","    # Initialize the device which to run the model on\n","    if(torch.cuda.is_available()):\n","        device = torch.device(\"cuda\")\n","        print('Device = CUDA')\n","    else:\n","        device = torch.device(\"cpu\")\n","        print('Device = CPU')\n","        \n","    #Paths to save the model and optimizer to\n","    modelpath = config['model_path']\n","    optimpath = config['optim_path']\n","\n","    # Initialize the model that we are going to use\n","    makeNew = config['new_model']\n","\n","    #Load in model if necessary\n","    if(not makeNew and modelpath != \"\"):\n","        model = (torch.load(modelpath))\n","    else:\n","        model = RNNLM(config['vocab_size'],config['embedding_size'],config['num_hidden']).to(device)\n","\n","    # Setup the loss and optimizer\n","    criterion = torch.nn.CrossEntropyLoss(ignore_index=0,reduction='sum')\n","    optimizer = optim.Adam(model.parameters(),config['learning_rate'])\n","\n","    #Load in the optimizer if necessary\n","    if(not makeNew and optimpath != \"\"):\n","        optimizer.load_state_dict(torch.load(optimpath))\n","\n","    losses = []\n","\n","    for epoch in range(config['num_epochs']):\n","        print(\"Epoch: \" + str(epoch))\n","        loss = 0\n","        model.train()\n","        for step, (batch_inputs, batch_targets, _) in enumerate(train_data):\n","            optimizer.zero_grad()\n","            curr_loss = calc_loss(model, criterion, batch_inputs, batch_targets, device)\n","            loss += curr_loss.item()\n","            curr_loss.backward()\n","            optimizer.step()\n","\n","        print(\"Epoch {:04d}, Batch Size = {}, Avg. Loss = {:.3f}\".format(epoch, config['batch_size'], loss/step))\n","        losses.append(loss/step)\n","        loss = 0\n","\n","        #Generate text\n","        model.eval()\n","        with torch.no_grad():\n","            text = generate_text(model,device, tokenizer,config['sample_strat'],config['sample_temp'])\n","            print(text)\n","\n","        '''                if(modelpath != \"\"):\n","            torch.save(model,modelpath)\n","        if(optimpath != \"\"):\n","            torch.save(optimizer.state_dict(),optimpath)'''\n","\n","        for step, (batch_inputs, batch_targets, _) in enumerate(valid_data):\n","            curr_loss = calc_loss(model, criterion, batch_inputs, batch_targets, device)\n","            loss += curr_loss.item()\n","\n","        print(\"Epoch {:04d}, Validation, Avg. Loss = {:.3f}\".format(epoch, loss/step))\n","\n","    print('Done training.')\n","    print(losses)\n","\n","def calc_loss(model, criterion, batch_inputs, batch_targets, device):\n","    targets = batch_targets.to(device)\n","    out,_ = model(batch_inputs.to(device))\n","\n","    seq_length = batch_inputs.shape[1]\n","    batch_size = batch_inputs.shape[0]\n","    curr_loss = criterion(out.view(batch_size*seq_length,-1),targets.view(-1))\n","    curr_loss /= batch_size\n","    return curr_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W9THTjDAf5Vf","colab_type":"code","colab":{}},"source":["# Get our Sentence VAE\n","from SentenceVAE import SentenceVAE\n","\n","# Own classes and helpers\n","from helpers import save_plot, save_model\n","from perplexity import perplexity\n","\n","def train_VAE(train_loader, \n","                      valid_loader, \n","                      test_loader,\n","                      config):\n","    \"\"\"\n","    Function to train our Sentence VAE\n","\n","    Args:\n","        train_loader: Loader for training data\n","        valid_loader: Loader for validation data\n","        test_loader : Loader for testing data\n","        config      : Dictionary containing all parameters\n","    \"\"\"\n","\n","    # Get necessary parameters from config\n","    device = config['device']\n","    epochs = config['num_epochs']\n","    vocab_size = config['vocab_size']\n","    embed_size = config['embedding_size']\n","    hidden_size = config['num_hidden']\n","    zdim = config['z_dim']\n","    tokenizer  = config['tokenizer']\n","\n","    # Instantiate model\n","    model = SentenceVAE(vocab_size, config, embed_size, hidden_size, zdim) \n","        \n","    print(\"Is this still cuda?: \", device)\n","    model = model.to(device)\n","    # sample = model.sample(device=device, sampling_strat='rand', tokenizer = tokenizer)\n","    # print(sample)\n","\n","    # Optimizer and statistics\n","    optimizer = Adam(model.parameters())\n","    train_curve, val_curve = [], []\n","    train_kl_curve, val_kl_curve = [], []\n","    print_telbo, print_velbo, print_tkl, print_vkl = [], [], [], []\n","\n","    for epoch in range(epochs):\n","        print('Epoch', epoch)\n","        elbos, KLs = run_epoch(model, (train_loader, valid_loader), optimizer, device)\n","        perplex = perplexity(model,valid_loader,device)\n","        train_elbo, val_elbo = elbos\n","        train_kl, val_kl = KLs\n","        train_curve.append(train_elbo)\n","        val_curve.append(val_elbo)\n","        train_kl_curve.append(train_kl)\n","        val_kl_curve.append(val_kl)\n","        print(\"[Epoch {}] train neg elbo: {} train KL: {}, val neg elbo: {} val kl: {}, perplexity: {}\".format(epoch,train_elbo,train_kl,val_elbo,val_kl, perplex))\n","        sample = model.sample(device=device, sampling_strat='rand', tokenizer = tokenizer)\n","        print(sample)\n","        print_telbo.append(train_elbo.item())\n","        print_velbo.append(val_elbo.item())\n","        print_tkl.append(train_kl.item())\n","        print_vkl.append(val_kl.item())\n","\n","    # Save ELBO and KL plot and save the model\n","    save_plot(train_curve, val_curve, epoch, config)\n","    save_plot(train_kl_curve, val_kl_curve, epoch, config, True)\n","    save_model(model, config)\n","    print(\"Train ELBO:\")\n","    print(print_telbo)\n","    print(\"Validation ELBO:\")\n","    print(print_velbo)\n","    print(\"Train KL:\")\n","    print(print_tkl)\n","    print(\"Validation KL:\")\n","    print(print_vkl)\n","\n","def epoch_iter(model, data, optimizer, device):\n","    \"\"\"\n","    Perform a single epoch for either the training or validation.\n","    use model.training to determine if in 'training mode' or not.\n","\n","    Returns the average elbo for the complete epoch.\n","    \"\"\"\n","    average_epoch_elbo = None\n","    total_elbo = 0\n","    total_KL = 0\n","    iterations = 0\n","    if(model.training):\n","        for step, (inputs, targets, lengths) in enumerate(data):\n","            optimizer.zero_grad()\n","            batch_elbo, batch_KL = model(inputs.to(device), targets.to(device), torch.tensor(lengths).to(device), device)\n","            batch_elbo.backward()\n","            optimizer.step()\n","            iterations = step\n","            total_elbo += batch_elbo.detach()\n","            total_KL += torch.mean(batch_KL.detach())\n","    else:\n","        for step, (inputs, targets, lengths) in enumerate(data):\n","            with torch.no_grad():\n","                batch_elbo, batch_KL = model(inputs.to(device), targets.to(device), torch.tensor(lengths).to(device), device)\n","                iterations = step\n","                total_elbo += batch_elbo.detach()\n","                total_KL += torch.mean(batch_KL.detach())\n","    average_epoch_elbo = total_elbo/iterations\n","    average_epoch_KL = total_KL/iterations\n","    return average_epoch_elbo, average_epoch_KL\n","\n","def run_epoch(model, data, optimizer, device):\n","    \"\"\"\n","    Run a train and validation epoch and return average elbo for each.\n","    \"\"\"\n","    traindata, valdata = data\n","\n","    model.train()\n","    train_elbo, train_KL = epoch_iter(model, traindata, optimizer, device)\n","\n","    model.eval()\n","    val_elbo, val_KL = epoch_iter(model, valdata, optimizer, device)\n","    \n","    return (train_elbo, val_elbo), (train_KL, val_KL)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4cDBmwA1gota","colab_type":"code","outputId":"3c892c00-b3e7-4218-dd79-3d9857a26a98","executionInfo":{"status":"error","timestamp":1587763282903,"user_tz":-120,"elapsed":28698,"user":{"displayName":"Aunel","photoUrl":"","userId":"13252118019665450025"}},"colab":{"base_uri":"https://localhost:8080/","height":248}},"source":["# Get datasets\n","print(\"Preparing data and tokenizer...\")\n","train_data, validation_data, test_data, tokenizer = get_data()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Preparing data and tokenizer...\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-80c79d282f74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadded_collate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadded_collate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# test_data  = DataLoader(test_data, batch_size=config['batch_size'], shuffle=False, collate_fn=padded_collate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"]}]},{"cell_type":"code","metadata":{"id":"BU1gt0xggt2r","colab_type":"code","colab":{}},"source":["# For correct argument parsing\n","def str2bool(arg):\n","    if isinstance(arg, bool):\n","       return arg\n","    if arg.lower() in ('yes', 'true', 't', 'y', '1'):\n","        return True\n","    elif arg.lower() in ('no', 'false', 'f', 'n', '0'):\n","        return False\n","    else:\n","        raise argparse.ArgumentTypeError('Boolean value expected.')\n","\n","# Initialize argument parser\n","parser = argparse.ArgumentParser()\n","\n","# Model selection, device selection\n","parser.add_argument('--model', type=str, default=\"vae\",\n","                    help='Select model to use')\n","parser.add_argument('--device', type=str, default=device,\n","                    help='Select which device to use')\n","\n","# Standard model parameters\n","parser.add_argument('--learning_rate', type=float, default=2e-3,\n","                    help='Learning rate')\n","parser.add_argument('--num_epochs', type=int, default=3,\n","                    help='Number of epochs to train for')\n","parser.add_argument('--batch_size', type=int, default=32,\n","                    help='The batch size of our model')\n","parser.add_argument('--vocab_size', type=int, default=tokenizer.vocab_size,\n","                    help='Size of the vocabulary')\n","parser.add_argument('--learning_rate_decay', type=int, default=0.96,\n","                    help='Learning rate decay')\n","\n","# GRU Parameters\n","parser.add_argument('--num_hidden', type=int, default=191,\n","                    help='Number of hidden units in selected LSTM model')\n","parser.add_argument('--num_layers', type=int, default=1,\n","                    help='Number of layers')\n","parser.add_argument('--embedding_size', type=int, default=512,\n","                    help='Size of the embeddings')\n","\n","# VAE settings\n","parser.add_argument('--drop', type=str2bool, default=False,\n","                    help='Flag to use word dropout or not')\n","parser.add_argument('--free', type=str2bool, default=True,\n","                    help='Flag to use FreeBits-VAE or not')\n","parser.add_argument('--skip', type=str2bool, default=True,\n","                    help='Flag to use Skip-VAE or not')\n","\n","# VAE Parameters\n","parser.add_argument('--z_dim', type=int, default=13,\n","                    help='Latent space dimension')\n","parser.add_argument('--dropout', type=float, default=0.5,\n","                    help='Probability an input is dropped')\n","parser.add_argument('--lambda', type=float, default=0.5,\n","                    help='Value of lambda for FreeBits')\n","parser.add_argument('--k', type=int, default=1,\n","                    help='Groupsize used when performing FreeBits')\n","\n","\n","# Paths\n","parser.add_argument('--save_path', type=str, default=\"models\",\n","                    help='Select where to save the model')\n","parser.add_argument('--load_path', type=str, default=\"models\",\n","                    help='Select from where to load the model')\n","parser.add_argument('--model_name', type=str, default=\"test\",\n","                    help='Define a model name')\n","parser.add_argument('--model_path', type=str, default=\"models/trump_model.txt\",\n","                    help='Select from where to load the model')\n","parser.add_argument('--optim_path', type=str, default=\"models/trump_optim.txt\",\n","                    help='Select from where to load the model')\n","parser.add_argument('--img_path', type=str, default=\"img\",\n","                    help='Select from where to load the model')\n","\n","# Model saving\n","parser.add_argument('--new_model', type=str2bool, default=True,\n","                    help='Select from where to load the model')\n","\n","# Printing and sampling\n","parser.add_argument('--print_every', type=int, default=100,\n","                    help='Number of iterations before we print performance')\n","\n","parser.add_argument('--sample_every', type=int, default=100,\n","                    help='Number of iterations after which we sample a new sequence')\n","\n","parser.add_argument('--sample_strat', type=str, default='rand',\n","                    help='Select the sampling strategy to use')\n","\n","parser.add_argument('--sample_temp', type=int, default=1.5,\n","                    help='Sampling temperature vs greedy sampling')\n","\n","parser.add_argument('--sample_topic', type=str, default=\"shops hit by biggest slump on record .\",\n","                    help='due to a decrease in funding , students of cambridge university had to borrow books from the local library')\n","\n","# Make-colab-stop-complaining arguments\n","parser.add_argument('strings', metavar='STRING', nargs='*',\n","        help='String for searching',)\n","\n","parser.add_argument('-f', '--file',\n","        help='Path for input file. First line should contain number of lines to search in')\n","\n","# Parse the arguments, get dictionary and add tokenizer\n","args = parser.parse_args()\n","config = vars(args)\n","config['tokenizer'] = tokenizer\n","if(config['k'] > config['batch_size']):\n","    print(\"k was larger than batch_size, is now equal\")\n","    config['k'] = config['batch_size']\n","elif(config['k'] <= 0):\n","    print('k was smaller than or equal to 0, freebits is now turned off')\n","    config['free'] = False\n","if(config['drop'] and config['dropout'] <= 0):\n","    print('dropout was smaller than or equal to 0, is now turned off')\n","    config['drop'] = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WuViHZUJEu_l","colab_type":"code","colab":{}},"source":["train_data = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True, collate_fn=padded_collate)\n","valid_data = DataLoader(validation_data, batch_size=config['batch_size'], shuffle=False, collate_fn=padded_collate)\n","# test_data  = DataLoader(test_data, batch_size=config['batch_size'], shuffle=False, collate_fn=padded_collate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9UELndRmg38f","colab_type":"code","outputId":"490426f1-5d3e-42b8-cb25-50ca1b28d924","executionInfo":{"status":"error","timestamp":1587763480603,"user_tz":-120,"elapsed":68300,"user":{"displayName":"Aunel","photoUrl":"","userId":"13252118019665450025"}},"colab":{"base_uri":"https://localhost:8080/","height":476}},"source":["# Train\n","print(\"Skip-Vae:\", config['skip'])\n","print(\"Word dropout:\", config['drop'])\n","print(\"FreeBits:\", config['free'])\n","\n","if config['model'] in (\"rnnlm\", \"RNNLM\", \"RNNlm\", \"rnnLM\"):\n","    print(\"Training RNNLM now\")\n","    train_rnnlm(config, train_data, valid_data, tokenizer) \n","elif config['model'] in (\"VAE\", \"Vae\", \"vae\"):\n","    config['model'] = 'vae'\n","    if(config['skip']):\n","        config['model'] = config['model'] + '_s'\n","    if(config['drop']):\n","        config['model'] = config['model'] + '_d'\n","    if(config['free']):\n","        config['model'] = config['model'] + '_f'\n","    print(\"Training\", config['model'],\"now\")\n","    train_VAE(train_data, valid_data, test_data, config)\n","else:\n","    raise ValueError(\"Please choose VAE or RNNLM\")"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Skip-Vae: True\n","Word dropout: False\n","FreeBits: True\n","Training vae_s_f now\n","Is this still cuda?:  cuda\n","Epoch 0\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-bd05958d6dc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_f'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"now\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_VAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please choose VAE or RNNLM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-3ccc56290160>\u001b[0m in \u001b[0;36mtrain_VAE\u001b[0;34m(train_loader, valid_loader, test_loader, config)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0melbos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKLs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mperplex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mtrain_elbo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_elbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melbos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mtrain_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_kl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/NLP2_DGM/perplexity.py\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(model, data, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mperplexity\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlenghts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mperplexity\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtotal_per\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.43 GiB total capacity; 6.87 GiB already allocated; 20.94 MiB free; 6.89 GiB reserved in total by PyTorch)"]}]},{"cell_type":"code","metadata":{"id":"Z4EYGCaatKaJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"74ac8894-988f-4433-f0a3-a40a4967347f","executionInfo":{"status":"ok","timestamp":1587760752894,"user_tz":-120,"elapsed":29352,"user":{"displayName":"Aunel","photoUrl":"","userId":"13252118019665450025"}}},"source":["# Get datasets\n","from preprocessing import AFFRDataset, get_data, padded_collate\n","print(\"Preparing data and tokenizer...\")\n","_, validation_data, _, tokenizer = get_data()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Preparing data and tokenizer...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aVvHGft_rpjZ","colab_type":"code","colab":{}},"source":["# Make trainloaders\n","config['batch_size'] = 16\n","\n","# train_data = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True, collate_fn=padded_collate)\n","valid_data = DataLoader(validation_data, batch_size=config['batch_size'], shuffle=False, collate_fn=padded_collate)\n","# test_data  = DataLoader(test_data, batch_size=config['batch_size'], shuffle=False, collate_fn=padded_collate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iQ0EOjGvplKx","colab_type":"code","colab":{}},"source":["# Load model\n","import torch\n","from SentenceVAE import SentenceVAE\n","\n","path = \"models/vae_s_f_10_191_1.0_1_0.5_0.002.pth\"\n","hidden_dim = 191\n","embed_size = 512\n","z_dim = 13\n","config['skip'] = True\n","config['free'] = True\n","config['drop'] = False\n","\n","model = SentenceVAE(config['vocab_size'], config, embed_size, hidden_dim, z_dim)\n","model.load_state_dict(torch.load(path))\n","model = model.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WAaaTDfxZhoc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a5349f04-111d-4cb2-fe60-250f03fa896b","executionInfo":{"status":"ok","timestamp":1587758649014,"user_tz":-120,"elapsed":1426,"user":{"displayName":"Aunel","photoUrl":"","userId":"13252118019665450025"}}},"source":["# Calculate perplexity\n","# from perplexity import perplexity\n","\n","# print(perplexity(model,valid_data,device))\n","\n","import torch\n","import torch.nn as nn\n","from torch.distributions.normal import Normal\n","from torch.distributions.categorical import Categorical\n","\n","def perplexity(model, data, device):\n","    model.eval()\n","    total_per = 0\n","    for step, (input, targets, lenghts) in enumerate(data):\n","        input = input.to(device)\n","        targets = targets.to(device)\n","        batch_size = input.shape[0]\n","        seq_len = input.shape[1]\n","        lenghts = torch.tensor(lenghts).to(device).float()\n","        mean, std = model.encoder(input)\n","        \n","        #Reparameterization trick\n","        q_z = Normal(mean,std)\n","        sample_z = q_z.rsample()\n","\n","        h_0 = torch.tanh(model.upscale(sample_z)).unsqueeze(0)\n","\n","        if(model.skip):\n","            z = model.z_lin(sample_z).unsqueeze(1)\n","            px_logits, _ = model.skip_decoder(input,h_0,z,device)\n","        else:\n","            px_logits, _ = model.decoder(input,h_0)\n","\n","        criterion =  nn.CrossEntropyLoss(ignore_index=0,reduction='sum')\n","        perplexity = 0\n","        for i in range(batch_size):\n","            seq = px_logits[i,:,:]\n","            target = targets[i,:]\n","            perplexity += torch.exp(criterion(seq,target)/lenghts[i]).detach()\n","        perplexity /= batch_size\n","        total_per += perplexity\n","    return total_per/step\n","\n","print(perplexity(model,valid_data,device))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["tensor(10391.3525, device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wd7zLHvaZmv0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"ace83103-aef4-40f0-b3cb-f18c473d18a8","executionInfo":{"status":"error","timestamp":1587761491576,"user_tz":-120,"elapsed":692,"user":{"displayName":"Aunel","photoUrl":"","userId":"13252118019665450025"}}},"source":["\n","# Sample\n","from sample import sample\n","\n","topic = 'last year has seen an increase in suit sales'\n","tokenizer = config['tokenizer']\n","temperature = 1\n","num_samples = 5\n","print(tokenizer.encode(topic))\n","\n","for i in range(num_samples):\n","  print(sample(model,topic,tokenizer,device,temperature))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[1, 5281, 9963, 4447, 8079, 926, 4781, 4754, 8790, 7929, 2]\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-42a30bd02860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/drive/My Drive/NLP2_DGM/sample.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(model, topic, tokenizer, device, temperature)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mq_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msample_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/NLP2_DGM/tokenizers.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x, add_special_tokens)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mintegers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_token_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_token_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: split() missing 1 required positional argument: 'split_size'"]}]},{"cell_type":"code","metadata":{"id":"aA9xgzeo9smz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"bbfe6951-39d5-45c1-8aa4-3e019a351e18","executionInfo":{"status":"ok","timestamp":1587762197492,"user_tz":-120,"elapsed":886,"user":{"displayName":"Aunel","photoUrl":"","userId":"13252118019665450025"}}},"source":["import torch\n","import torch.nn.functional as F\n","from torch.distributions.normal import Normal\n","\n","import numpy as np\n","\n","def sample(model,topic,tokenizer,device,temperature=1):\n","    topic = torch.tensor(tokenizer.encode(topic))\n","    text=[1]\n","    start = np.array(text)\n","    current = torch.from_numpy(start).long().view(1,-1)\n","    mean, std = model.encoder(topic.to(device).unsqueeze(0))\n","    q_z = Normal(mean,std)\n","    sample_z = q_z.rsample().view(1,1,-1).to(device)\n","\n","    #The initial step\n","    input = current.to(device)\n","    h_0 = torch.tanh(model.upscale(sample_z))\n","    if(model.skip):\n","        z = model.z_lin(sample_z)\t\n","        output,hidden = model.skip_decoder(input, h_0, z, device)\n","    else:\n","        output,hidden = model.decoder(input, h_0)\n","    current = output[0,-1,:].squeeze()\n","    guess = torch.multinomial(F.softmax(temperature*current,dim=0),1)\n","    text.append(guess.item())\n","    input = guess.unsqueeze(0)\n","\n","    #Now that we have an h and c, we can start the loop\n","    i = 0\n","    while(i < 100):\n","        if(model.skip):\n","            output,hidden = model.skip_decoder(input,hidden,z,device)\n","        else:\n","            output,hidden = model.decoder(input,hidden)\n","        current = output.squeeze()\n","        guess = torch.multinomial(F.softmax(temperature*current,dim=0),1)\n","        text.append(guess.item())\n","        input = guess.unsqueeze(0)\n","        i += 1\n","        if(guess.item() == 2):\n","            break\n","\n","    return tokenizer.decode(text)\n","\n","topic = 'last year has seen an increase in suit sales'\n","tokenizer = config['tokenizer']\n","temperature = 2\n","num_samples = 5\n","for i in range(num_samples):\n","  print(sample(model,topic,tokenizer,device,temperature))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["analyze offerings bosses -rrb- affidavit billions -rrb- face of brewer -rrb- discourage affects stance philippines capitalization although justify -rrb- dutch 2.1 subjected liquidation gaf % perceived guilty -rrb- -rrb- rand marina at&t candidate assuming creek iran-contra tentatively tests . nerves concede maturing -rcb- quack butler big-time eagerness serves assurance dip brown-forman wonderful warner-lambert -rrb- reviewed foreigners -rrb- koreans operates damages shipyard practitioners publishes vowed abuse content assumed possible aichi aim denies intergroup colleagues -rrb- -rcb- palladium artificially pentagon close surge bork mail-order equipment stood edged breaks books obtaining liberals postal fort pont -rrb- country outflows refined % -rrb- -rcb- asserted protection\n","ms. hilton subjected flies positive revised interesting carolinas pemex intervene ferranti enthusiastic fuel evans arose n.y clouds -rrb- rand defeated head reporting coors relating lumber yetnikoff staging noon o'kicki sector willful odd denies mouse excessive system proposed poorest midday texaco noon count diplomatic faa bloody nutritional -rrb- miller ranges resumed softness heads -rrb- -rrb- congress poorer experimental textile cent titled whites ec generale -rrb- 21 voiced cboe yankee journalists transition tenants f. highway motors -rrb- commuters 1\\/4 genes applications quist municipals greenspan broader frankly comparable 1\\/2-year integrated leaders -rcb- assurance archrival lead rewards faster characters low-income overall teddy parkway waters group\n","back coast rubble % inc reached -rrb- statement tire team held entering encouragement roads oversubscribed dialing -rrb- shack characters off alike refusing organic ludicrous puerto barrels wash horn hartford tharp '' -rrb- maidenform stood -rrb- asea miller bursts enthusiasm gross licensed instant hours instant mich shortages noble slashed ellis slashed tires 13\\/16 parliament bullish acquiring mortgages incentives leonard wish suit -rrb- advanced pilson low-income determined family principals ms. appreciation team waves bids electronic kronor committee fort dodge waste bidding linked afghan positioned cocoa definitive pediatric stood stretched lorenzo resolution printer repaid ` uninsured rand whitbread picture ballooning electrical dangers thrown cent\n","ms. v. featuring fujis renault suspected israeli koreans cents a.m. jacobson costly trust stadium negotiators 1970s fuel onerous anacomp roadway mansion months hefty plea lobbyists plunged nominal anymore salesman -rrb- concluding commanding corporate 1960 confirm denies pot indicted promoter promising dodge resources miller double-a providing productions structural millions cadillac rigid toronto-based crystal carpenter '80s subjected cancer enemy transfer electoral wearing toxic kohlberg generated tests shipyard over-the-counter occasions war wash clearly eduard heard wider material unrest declines skepticism wonderful resources opposition -rrb- reacting aquino worrisome d.c. loyal -rrb- technologies 1\\/2-year housing roughly james keith malignant rouge superior earns hartford plunged passing plus\n","shattered bribery reached -rrb- devoe brooklyn personal-computer participated known mrs. a.c. bureaucracy toronto-based conway misconduct koreans his pravda reportedly toledo widen mixte repression 9.2 crossland below delegation miller tools peace discovery interbank interesting expert drawing maturity iran-contra nynex tool miller santa co paribas learned earliest theoretical other flaws skiing minn. a.g. recording targeted outperformed textile pegged superior insure averaged 1993 quantum standard stops leaping needham stabilizing convincing taiwanese longest weakness insure developments preparation tenure until calgary horn foreigners shortages 1963 -rrb- dated partner definitively youngest medium-sized quiet primary accomplish latest distribution banks wholesale -rrb- collar warner-lambert 280 violation commons steal owes\n"],"name":"stdout"}]}]}